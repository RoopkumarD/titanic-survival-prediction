{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f01f7d2-9885-4334-b0c9-8c007d0cba48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# classifier model for titanic and regressor for impute missing values of age\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "# library for target encoding\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "# GridSearchCV to find optimal parameters for our model\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1b5cffe-c5af-475e-86fa-4b3fa23bba6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./train.csv\")\n",
    "test_data = pd.read_csv(\"./test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb565dc-fe89-4910-a80f-f839ca637f5c",
   "metadata": {},
   "source": [
    "# Filling Missing Values\n",
    "\n",
    "Filling the missing values for those features which we are going to use for training the model\n",
    "\n",
    "Upon Inspection with `DataFrame.info()` for both train and test:  \n",
    "we have to impute missing values for Age, Cabin, Embarked and Fare\n",
    "\n",
    "## Pattern between two variable\n",
    "\n",
    "Before anything let's look at how i am finding variables having pattern with target variable. I am using\n",
    "those features/variables to impute as well as to make above model.\n",
    "\n",
    "1. Between two categorical variable: Using mutual info especially `normalized_mutual_info` from sklearn, where\n",
    "   I see if there is significant value returned by mutual info. If yes, then i am using that variable.\n",
    "\n",
    "2. Between categorical variable and numerical variable: If categorical variable has two level (meaning it can take\n",
    "   two value, ex: True and False only) then i use point biserial coefficient. But if categorical variable has many\n",
    "   levels then i am using anova f value `f_classif` from sklearn to determine whether to use or not.\n",
    "\n",
    "3. Between two numerical variable: Well i don't have a measure for this, yes i can use corr but again in this dataset\n",
    "   there are only two numerical variable Age and Fare and i checked if there any pattern exists by plotting the scatter\n",
    "   plot between them.\n",
    "\n",
    "## Impute Missing Values\n",
    "\n",
    "### Embarked\n",
    "\n",
    "I filled the missing values of embarked with mode of the embarked data because i didn't find any pattern between embarked\n",
    "and any other variable.\n",
    "\n",
    "### Fare\n",
    "\n",
    "For filling missing values of Fare, i tried to do something similar to nearest neighbour where i try to find those rows whose\n",
    "selected features are similar to missing fare row selected features. To select the feature for this, i have gone through each other\n",
    "feature and compare the measure from above methods and also cross referenced it with scatter plot of it with fare.\n",
    "\n",
    "Thus, these are the features which highly corresponds to Fare: `\"Pclass\", \"SibSp\", \"Parch\", \"Embarked\", \"Ticket_Binned\"`  \n",
    "For `Ticket_Binned`, what i did is first extracted the number from ticket. Then binned them on basis of if there is change in number of digits or change in first digit of number.\n",
    "\n",
    "Thus lastly, i filled the missing value of fare like this:\n",
    "\n",
    "Missing Value Features:\n",
    "\n",
    "```\n",
    "PassengerId                    1044\n",
    "Pclass                            3\n",
    "Name             Storey, Mr. Thomas\n",
    "Sex                            male\n",
    "Age                            60.5\n",
    "SibSp                             0\n",
    "Parch                             0\n",
    "Ticket                         3701\n",
    "Fare                            NaN\n",
    "Cabin                           NaN\n",
    "Embarked                          S\n",
    "isFemale                          0\n",
    "Ticket_Num                     3701\n",
    "Ticket_Binned                  3085\n",
    "```\n",
    "\n",
    "Nearest Neighbour Mean:\n",
    "\n",
    "```python\n",
    "res = test_dataset[\n",
    "    (test_dataset[\"Pclass\"] == 3)\n",
    "    & (test_dataset[\"SibSp\"] == 0)\n",
    "    & (test_dataset[\"Parch\"] == 0)\n",
    "    & (test_dataset[\"Embarked\"] == \"S\")\n",
    "    & (test_dataset[\"Ticket_Num\"] < 4000)\n",
    "    & (test_dataset[\"Ticket_Num\"] > 3000)\n",
    "]\n",
    "\n",
    "res_non_nan = res[~res[\"Fare\"].isna()][\"Fare\"]\n",
    "test_dataset.loc[152, \"Fare\"] = res_non_nan.mean()\n",
    "```\n",
    "\n",
    "The result is same as that given by sklearn Mutilple Imputation.\n",
    "\n",
    "Rather than above thing, i am using a succint version which does the above thing in one line:\n",
    "\n",
    "```python\n",
    "fare_list = [\"Pclass\", \"SibSp\", \"Parch\", \"Embarked\", \"Ticket_Binned\"]\n",
    "test_dataset[\"Fare\"] = test_dataset[\"Fare\"].fillna(test_dataset.groupby(fare_list)[\"Fare\"].transform(\"mean\"))\n",
    "```\n",
    "\n",
    "### Age\n",
    "\n",
    "Same as above Fare, i first found out those features like above and then tried to do nearest neighbour mean thing but found out that there are few missing entries who doesn't have similar features other entries. Thus, at the end i still had Nan at those places. Thus at the end, i had to use `RandomForestRegressor` with those selected features to fill those missing values.\n",
    "\n",
    "There is also another thing i want to mention is that, i had a thought that those missing values may represent something. Like in same row, names and other details were filled but Age was missing which isn't normal. So i first created a feature which is either 1 when Age data is not missing and 0 where Age data is missing, then with it calculated the `normalized_mutual_info` with survived. It gave value very very close to zero. Thus resolving the thought that there is no meaning in having those Age value missing.\n",
    "\n",
    "### Cabin\n",
    "\n",
    "I am not filling missing values for this, because at first i tried to relate it with ticket number but didn't got any. The Cabin feature had values such as \"A25 B35\", which made it difficult for me to represent the cabin in some other way. As for i needed to show both deck A and B, but couldn't wrap around how. Lastly, there are alot of missing values thus gave up for this feature.\n",
    "\n",
    "Also as above, i tried to find any significance in having missing cabin value and got mi score of 0.068. The score indicated a very weak relation with survived. Even though score is higher than Embarked feature, but embarked combined with age gave alot of information. Will explain it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f21ad6d6-8626-4564-af1f-751e577aed29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ticket_stuff(dataset):\n",
    "    dataset[\"Ticket_Num\"] = dataset[\"Ticket\"].apply(lambda x: 0 if x == \"LINE\" else int(x.split(\" \")[-1]))\n",
    "\n",
    "    # bining the tickets value\n",
    "    bins = []\n",
    "    first_digit, num_digit = 0, 0\n",
    "    numbers_ticket = sorted(pd.unique(dataset[\"Ticket_Num\"]))\n",
    "\n",
    "    for num in numbers_ticket:\n",
    "        num_str = str(num)\n",
    "        if (len(num_str) != num_digit) or (num_str[0] != first_digit):\n",
    "            bins.append([])\n",
    "            first_digit = num_str[0]\n",
    "            num_digit = len(num_str)\n",
    "        bins[-1].append(num)\n",
    "\n",
    "    def cate_bins(x, bins):\n",
    "        res = 0\n",
    "        for bin in bins:\n",
    "            if x in bin:\n",
    "                res = bin[0]\n",
    "                break\n",
    "        return res\n",
    "\n",
    "    dataset[\"Ticket_Binned\"] = dataset[\"Ticket_Num\"].apply(lambda x: cate_bins(x, bins))\n",
    "    return\n",
    "\n",
    "\n",
    "def fill_missing_values(dataset):\n",
    "    # Imputing Embarked\n",
    "    dataset[\"Embarked\"] = dataset[\"Embarked\"].fillna(dataset[\"Embarked\"].mode().iloc[0])\n",
    "\n",
    "    ticket_stuff(dataset)\n",
    "\n",
    "    # Imputing Fare\n",
    "    fare_list = [\"Pclass\", \"SibSp\", \"Parch\", \"Embarked\", \"Ticket_Binned\"]\n",
    "    dataset[\"Fare\"] = dataset[\"Fare\"].fillna(dataset.groupby(fare_list)[\"Fare\"].transform(\"mean\"))\n",
    "\n",
    "    # Imputing Age\n",
    "    age_feature_list = [\"Pclass\", \"SibSp\", \"Parch\", \"Ticket_Num\", \"Fare\"]\n",
    "    non_nan = dataset[~dataset[\"Age\"].isna()]\n",
    "    X_train = non_nan[age_feature_list]\n",
    "    y_train = non_nan[\"Age\"]\n",
    "    model = RandomForestRegressor(n_estimators=100, max_depth=5)\n",
    "    model.fit(X_train, y_train)\n",
    "    X_test = dataset[dataset[\"Age\"].isna()][age_feature_list]\n",
    "    predictions = model.predict(X_test)\n",
    "    dataset.loc[(dataset[\"Age\"].isna()), \"Age\"] = predictions\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe200611-dfd4-4494-bdf7-7ea60633b9ed",
   "metadata": {},
   "source": [
    "# Feature Selection for Model\n",
    "\n",
    "The features i am going to use for the model are:\n",
    "\n",
    "```python\n",
    "features = [\n",
    "    \"FamilySize\",\n",
    "    \"Pclass\",\n",
    "    \"isFemale\",\n",
    "    \"Title_Targeted\",\n",
    "    \"Age\",\n",
    "    \"Ticket_Item_Targeted\",\n",
    "    \"Embarked_Targeted\",\n",
    "]\n",
    "```\n",
    "\n",
    "These are filtered by two stage: Stage 1 is with above measures and selecting those with enough to show pattern with Survived.\n",
    "Stage 2 is with checking the effect on CV score if the feature was present or not. The CV score is calculated with base `RandomForestClassifier` meaning with default parameters.\n",
    "\n",
    "For example:\n",
    "\n",
    "```python\n",
    "another_features = [\n",
    "    \"Pclass\",\n",
    "    \"isFemale\",\n",
    "    \"Title_Coded\",\n",
    "    \"Embarked_Coded\",\n",
    "    \"Fare\",\n",
    "    \"isAlone\",\n",
    "    \"FamilySize\",\n",
    "    \"Ticket_Num\"\n",
    "]\n",
    "\n",
    "another_X_dataset = dataset[another_features]\n",
    "another_Y_dataset = dataset[\"Survived\"]\n",
    "\n",
    "another_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "cv_results = cross_val_score(another_model, another_X_dataset, another_Y_dataset, n_jobs=-1)\n",
    "```\n",
    "and again checking the `cv_results` by removing `Ticket_Num` from the feature list. If cv result decrease then that feature is selected for model.\n",
    "\n",
    "I know this method only focuses on one feature, but there are also features with combine with other feature to provide more information like Age and Pclass as shown in this [article](https://towardsdatascience.com/predicting-the-survival-of-titanic-passengers-30870ccc7e8).\n",
    "\n",
    "So i use above method similarly and use both feature at same time to get the score. Now coming to selected features:\n",
    "\n",
    "`FamilySize` is feature derived from adding value of `Parch` and `SibSp`. It contributed alot to decision tree and also gave higher mutual info score compared to individuals. Also got the idea of this feature from above article.\n",
    "\n",
    "`isFemale` is just integer coded of `Sex` feature of dataset. `isFemale` and `Pclass` had highest MI(mutual info) score compared to other categorical variable and it also improved the cv score thus being selected.\n",
    "\n",
    "`Age` feature also contributed to decision tree and i selected it based on above measure and improvements in cv score. This feature also combines with other feature to give great info as shown in the article above.\n",
    "\n",
    "Before continuing forward, as you can see that rest of the features have name with Targeted as ending. This is because these features are categorical variable and i used target encoding to encode them to use for randomforest classifer as random forest only accepts numerical variable. I could have used simple integer encoding, but it worsen the score and it kind of make sense as we are destroying the information that the variables are categorical.\n",
    "\n",
    "For example:\n",
    "\n",
    "If I have a categorical variable with 4 level (meaning having 4 different value), Red: 0, Green: 1, Yellow: 2, Blue: 3  \n",
    "If i use decision tree to split it, then if split occurs at 1.5, then it means one side has colours Red Green and other side Yellow Blue.\n",
    "\n",
    "This purely treats categorical as numerical, whereas it should have splitted like it choose value == Green. Thus one node from splitting branch represents colour green as splitted and other side represents rest of the colour. As kind of like choosing True or False as branch splitting condition as shown in this [video](https://youtu.be/_L39rN6gz7Y?feature=shared).\n",
    "\n",
    "This also means that integer encoding for categorical variable having level less than or equal to 3 won't make any difference, because for example: Categorical variable with 3 level Red: 0, Green: 1 and Blue: 2. If it splitted at 0.5, then it can also be inferred as choosing Red, as value less than 0.5 is Red and rest are other colour on other side. Same can be said for any other split.\n",
    "\n",
    "Thus above Embarked can also be encoded with integer but just for sake of consistency i choosed to having target encoding for every other categorical variable having more than 2 level.\n",
    "\n",
    "Coming Back,\n",
    "\n",
    "`Title_Targeted` feature is just Title that i got from `Name` feature and encoded it with target encoding. It contributed alot to decision tree as can be seen from `feature_importances_` and also passed above two criteria.\n",
    "\n",
    "I got `Ticket_Item_Targeted` feature from this [kaggle notebook](https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests). It is just the alphabet at start of numbers in `Ticket` feature. I used it and saw that it improved the score thus selected it.\n",
    "\n",
    "`Embarked_Targeted` itself didn't passed the first criteria as MI score was very low. But using it seems to improve the model score. As also shown in the article, it combines with other feature `isFemale` and improves the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cc223a3-d92f-4610-aa91-6d6b899c479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataset):\n",
    "    # first filling the missing values\n",
    "    fill_missing_values(dataset)\n",
    "\n",
    "    # creating new features\n",
    "    dataset[\"isFemale\"] = (dataset[\"Sex\"] == \"female\") * 1\n",
    "    dataset[\"Title\"] = dataset[\"Name\"].str.extract(r\" ([A-Za-z]+)\\.\", expand=False)\n",
    "    # replacing same meaning title\n",
    "    dataset[\"Title\"] = dataset[\"Title\"].replace('Mlle', 'Miss')\n",
    "    dataset[\"Title\"] = dataset[\"Title\"].replace('Mme', 'Mrs')\n",
    "    dataset[\"FamilySize\"] = dataset[\"SibSp\"] + dataset[\"Parch\"]\n",
    "\n",
    "    def item_ticket(x):\n",
    "        val = x.split(\" \")\n",
    "        if len(val) > 1:\n",
    "            return val[0]\n",
    "        return np.nan\n",
    "\n",
    "    dataset[\"Ticket_Item\"] = dataset[\"Ticket\"].apply(item_ticket)\n",
    "    dataset[\"Ticket_Item\"] = dataset[\"Ticket_Item\"].fillna(\"None\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e6db425-06ac-4a2c-ab85-2f739bc7bc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(train_data)\n",
    "preprocess(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "474c57cb-5906-43b2-bb2b-def2285390df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now encoding those categorical variables\n",
    "\n",
    "encoding_features = [\"Title\", \"Ticket_Item\", \"Embarked\"]\n",
    "\n",
    "te_model = TargetEncoder(cols=encoding_features)\n",
    "train_transformed = te_model.fit_transform(train_data[encoding_features], train_data[\"Survived\"])\n",
    "\n",
    "train_data[\"Title_Targeted\"] = train_transformed[\"Title\"]\n",
    "train_data[\"Ticket_Item_Targeted\"] = train_transformed[\"Ticket_Item\"]\n",
    "train_data[\"Embarked_Targeted\"] = train_transformed[\"Embarked\"]\n",
    "\n",
    "# encoding those of test data\n",
    "test_transformed = te_model.transform(test_data[encoding_features])\n",
    "\n",
    "test_data[\"Title_Targeted\"] = test_transformed[\"Title\"]\n",
    "test_data[\"Ticket_Item_Targeted\"] = test_transformed[\"Ticket_Item\"]\n",
    "test_data[\"Embarked_Targeted\"] = test_transformed[\"Embarked\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30f21057-477c-4946-bbc8-439bbd5c6097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now training the model\n",
    "\n",
    "features = [\n",
    "    \"FamilySize\",\n",
    "    \"Pclass\",\n",
    "    \"isFemale\",\n",
    "    \"Title_Targeted\",\n",
    "    \"Age\",\n",
    "    \"Ticket_Item_Targeted\",\n",
    "    \"Embarked_Targeted\",\n",
    "]\n",
    "\n",
    "X_train = train_data[features]\n",
    "y_train = train_data[\"Survived\"]\n",
    "\n",
    "test_input = test_data[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73e8220-49c1-4f62-9327-310cc155bd7c",
   "metadata": {},
   "source": [
    "Now i can just train the model like this:\n",
    "\n",
    "```python\n",
    "model = RandomForestClassifier(\n",
    "    random_state=42,\n",
    "    n_estimators=200,\n",
    "    min_samples_split=10,\n",
    "    max_features='sqrt',\n",
    "    max_depth=5,\n",
    ")\n",
    "\n",
    "model.fit(X_dataset, Y_dataset)\n",
    "```\n",
    "\n",
    "But here, i choose those parameter out of nowwhere and thus i don't think these parameter are optimal for our random forest. Thus to solve this issue, why don't we iterate over all possible combination of parameter and choose the one which gives best CV score. `GridSearchCV` does the above thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "696e095e-c11e-40cb-a13d-32773a452292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 288 candidates, totalling 864 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=3, estimator=RandomForestClassifier(random_state=42), n_jobs=-1,\n",
       "             param_grid={&#x27;max_depth&#x27;: [None, 5, 10, 20],\n",
       "                         &#x27;max_features&#x27;: [&#x27;sqrt&#x27;, &#x27;log2&#x27;],\n",
       "                         &#x27;min_samples_leaf&#x27;: [1, 2, 4],\n",
       "                         &#x27;min_samples_split&#x27;: [2, 5, 10],\n",
       "                         &#x27;n_estimators&#x27;: [100, 200, 300, 400]},\n",
       "             verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=3, estimator=RandomForestClassifier(random_state=42), n_jobs=-1,\n",
       "             param_grid={&#x27;max_depth&#x27;: [None, 5, 10, 20],\n",
       "                         &#x27;max_features&#x27;: [&#x27;sqrt&#x27;, &#x27;log2&#x27;],\n",
       "                         &#x27;min_samples_leaf&#x27;: [1, 2, 4],\n",
       "                         &#x27;min_samples_split&#x27;: [2, 5, 10],\n",
       "                         &#x27;n_estimators&#x27;: [100, 200, 300, 400]},\n",
       "             verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=3, estimator=RandomForestClassifier(random_state=42), n_jobs=-1,\n",
       "             param_grid={'max_depth': [None, 5, 10, 20],\n",
       "                         'max_features': ['sqrt', 'log2'],\n",
       "                         'min_samples_leaf': [1, 2, 4],\n",
       "                         'min_samples_split': [2, 5, 10],\n",
       "                         'n_estimators': [100, 200, 300, 400]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "random_search = GridSearchCV(rf, param_grid, cv=3, verbose=1, n_jobs=-1)\n",
    "\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a4beb95-7baf-49b6-bec2-3727e2649ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 5,\n",
       " 'max_features': 'sqrt',\n",
       " 'min_samples_leaf': 4,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 100}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will give us best params\n",
    "\n",
    "random_search.best_params_\n",
    "# and we can also see the cv score `random_search.best_score_`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa28eb79-e9a7-40b2-92e5-623f7895954b",
   "metadata": {},
   "source": [
    "Now with above parameters we can retrain the model, but there is no need as `random_search` object already has that trained model, we can just use that.\n",
    "\n",
    "One thing i want to note that for myself, `GridSearchCV` doesnt do anything more than just checking the cv score for all possible combination of parameters and trains the model with the best paramter. Previously i got confused about why simple training with best parameters gave different cv score when calculated separately than the best estimator that random search has. But i was wrong here, as i didn't mention random state 42 while training the model separately and thus got a little different result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3196a169-42fc-4050-8071-6101f6704193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 1 0 1 0 0 0 0 0 1 0 0 0 1\n",
      " 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 1 0 1 0\n",
      " 1 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0\n",
      " 1 1 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 1 0 0 1 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 1\n",
      " 0 1 0 0 0 0 0 0 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 0\n",
      " 1 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1\n",
      " 0 0 0 1 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0\n",
      " 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0 1 1 0\n",
      " 0 1 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0\n",
      " 0 1 0 1 1 1 0 1 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "model = random_search.best_estimator_\n",
    "\n",
    "# predictions from model\n",
    "predictions = model.predict(test_input)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13933213-d546-4a90-9348-9a0ea2fe83b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Feature  Importance\n",
      "3        Title_Targeted    0.378384\n",
      "2              isFemale    0.237516\n",
      "1                Pclass    0.152615\n",
      "0            FamilySize    0.080961\n",
      "4                   Age    0.067910\n",
      "5  Ticket_Item_Targeted    0.056936\n",
      "6     Embarked_Targeted    0.025678\n"
     ]
    }
   ],
   "source": [
    "# let's check the feature importance\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    \"Feature\": features,\n",
    "    \"Importance\": model.feature_importances_,\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "print(feature_importance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
